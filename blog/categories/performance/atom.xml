<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Performance | Postgres and other musings]]></title>
  <link href="http://gurjeet.singh.im/blog/categories/performance/atom.xml" rel="self"/>
  <link href="http://gurjeet.singh.im/blog/"/>
  <updated>2014-07-23T23:03:27+00:00</updated>
  <id>http://gurjeet.singh.im/blog/</id>
  <author>
    <name><![CDATA[Gurjeet Singh]]></name>
    <email><![CDATA[gurjeet@singh.im]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Announcing TPC-C.js; a Lightweight Implementation of TPC-C]]></title>
    <link href="http://gurjeet.singh.im/blog/2014/07/23/announcing-tpc-c-dot-js-a-lightweight-implementation-of-tpc-c/"/>
    <updated>2014-07-23T22:53:08+00:00</updated>
    <id>http://gurjeet.singh.im/blog/2014/07/23/announcing-tpc-c-dot-js-a-lightweight-implementation-of-tpc-c</id>
    <content type="html"><![CDATA[<p>I am glad to announce the <a href="https://github.com/gurjeet/DBYardstick/tree/v0.1.0/TPC-C">beta</a> release of TPC-C.js, which implements one of
the most popular database benchmarks, <a href="http://www.tpc.org/tpcc/default.asp">TPC-C</a>. It&rsquo;s not a coincidence that today
is also the <a href="http://www.tpc.org/information/sessions/sigmod/sld007.htm">22nd anniversary</a> of the TPC-C benchmark.</p>

<p>It currently supports <a href="http://www.postgresql.org/">Postgres</a> database, but can be easily extended to test
other database systems.</p>

<p>You might ask <em>&ldquo;Why another TPC-C implementation when we already have so many of
them?&rdquo;&ldquo;</em></p>

<p>Short answer: This one is very light on system resources, so you can</p>

<ol>
<li>Run the benchmark strictly adhering to the specification, and</li>
<li>Invest more in database hardware, rather than client hardware.</li>
</ol>


<p>Long answer: It&rsquo;s covered in the <a href="https://github.com/gurjeet/DBYardstick/tree/master/TPC-C#motivation">Motivation</a> section of TPC-C.js, which I&rsquo;ll
quote here:</p>

<blockquote><h1>Motivation</h1>

<p>The TPC-C benchmark drivers currently available to us, like TPCC-UVa, DBT2,
HammerDB, BenchmarkSQL, etc., all run one process (or thread) per simulated
client. Because the TPC-C benchmark specification limits the max tpmC metric
(transactions per minute of benchmark-C) from any single client to be 1.286 tpmC,
this means that to get a result of, say, 1 million tpmC we have to run about
833,000 clients. Even for a decent number as low as 100,000 tpmC, one has to run
83,000 clients.</p>

<p>Given that running a process/thread, even on modern operating systems, is a bit
expensive, it requires a big upfront investment in hardware to run the thousands
of clients required for driving a decent tpmC number. For example, the current
TPC-C record holder had to run 6.8 million clients to achieve 8.55 million tpmC,
and they used 16 high-end servers to run these clients, which cost them about
$ 220,000 (plus $ 550,000 in client-side software).</p>

<p>So, to avoid those high costs, these existing open-source implementations of
TPC-C compromise on the one of the core requirements of the TPC-C benchmark:
keying and thinking times. These implementations resort to just hammering the
SUT (system under test) with a constant barrage of transactions from a few
clients (ranging from 10-50).</p>

<p>So you can see that even though a decent modern database (running on a single
machine) can serve a few hundred clients simultaneously, it ends up serving
very few (10-50) clients. I strongly believe that this way the database is
not being tested to its full capacity; at least not as the TPC-C specification
intended.</p>

<p>The web-servers of yesteryears also suffer from the same problem; using one
process for each client request prohibits them from scaling, because the
underlying operating system cannot run thousands of processes efficiently. The
web-servers solved this problem (known as <a href="http://en.wikipedia.org/wiki/C10k_problem">c10k problem</a>) by using event-driven
architecture which is capable of handling thousands of clients using a single
process, and with minimal effort on the operating system&rsquo;s part.</p>

<p>So this implementation of TPC-C uses a similar architecture and uses <a href="http://nodejs.org/">NodeJS</a>,
the event-driven architecture, to run thousands of clients against a database.</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introducing Postgres Hibernator]]></title>
    <link href="http://gurjeet.singh.im/blog/2014/02/03/introducing-postgres-hibernator/"/>
    <updated>2014-02-03T21:28:46+00:00</updated>
    <id>http://gurjeet.singh.im/blog/2014/02/03/introducing-postgres-hibernator</id>
    <content type="html"><![CDATA[<p>As it must have been obvious from my last <a href="http://gurjeet.singh.im/blog/2014/01/21/hibernating-and-restoring-postgres-buffer-cache/">post</a> that I
wasn&rsquo;t really pleased by the amount of work needed to implement hibernation of
Postgres shared-buffers, so I set out to implement a seamless Postgres hibernation
solution.</p>

<p>A couple of hours ago I <a href="http://www.postgresql.org/message-id/CABwTF4Ui_anAG+ybseFunAH5Z6DE9aw2NPdy4HryK+M5OdXCCA@mail.gmail.com">published</a> the <a href="http://www.postgresql.org">Postgres</a>/<a href="http://www.enterprisedb.com">EDB</a> extension I had been
working on for last 10 days or so, in my spare time. Following are the contents of the
README file from the <a href="https://github.com/gurjeet/pg_hibernate">extension</a>.</p>

<h1>Postgres Hibernator</h1>

<p>This Postgres extension is a set-it-and-forget-it solution to save and restore
the Postgres shared-buffers contents, across Postgres server restarts.</p>

<p>For some details on the internals of this extension, also see the <a href="http://www.postgresql.org/message-id/CABwTF4Ui_anAG+ybseFunAH5Z6DE9aw2NPdy4HryK+M5OdXCCA@mail.gmail.com">proposal</a>
email to Postgres hackers' mailing list.</p>

<h2>Why</h2>

<p>When a database server is shut down, for any reason (say, to apply patches, for
scheduled maintenance, etc.), the active data-set that is cached in memory by
the database server is lost. Upon starting up the server again, the database
server&rsquo;s cache is empty, and hence almost all application queries respond slowly
because the server has to fetch the relevant data from the disk. It takes quite a
while for the server to bring the cache back to similar state as before the server
shutdown.</p>

<p>The duration for which the server is building up caches, and trying to reach its
optimal cache performance is called ramp-up time.</p>

<p>This extension is aimed at reducing the ramp-up time of Postgres servers.</p>

<h2>How</h2>

<p>Compile and install the extension (of course, you&rsquo;d need Postgres installation or
source code):</p>

<pre><code>$ make -C pg_hibernate/ install
</code></pre>

<p>Then.</p>

<ol>
<li>Add <code>pg_hibernate</code> to the <code>shared_preload_libraries</code> variable in <code>postgresql.conf</code> file.</li>
<li>Restart the Postgres server.</li>
<li>You are done.</li>
</ol>


<h2>How it works</h2>

<p>This extension uses the <code>Background Worker</code> infrastructure of Postgres, which was
introduced in Postgres 9.3. When the server starts, this extension registers
background workers; one for saving the buffers (called <code>Buffer Saver</code>) when the
server shuts down, and one for each database in the cluster (called <code>Block Readers</code>)
for restoring the buffers saved during previous shutdown.</p>

<p>When the Postgres server is being stopped/shut down, the <code>Buffer Saver</code> scans the
shared-buffers of Postgres, and stores the unique block identifiers of each cached
block to the disk (with some optimizatins). This information is saved under the
<code>$PGDATA/pg_hibernate/</code> directory. For each of the database whose blocks are
resident in shared buffers, one file is created; for eg.:
<code>$PGDATA/pg_hibernate/2.postgres.save</code>.</p>

<p>During the next startup sequence, the <code>Block Reader</code> threads are registerd, one for
each file present under <code>$PGDATA/pg_hibernate/</code> directory. When the Postgres server
has reached stable state (that is, it&rsquo;s ready for database connections), these
<code>Block Reader</code> processes are launched. The <code>Block Reader</code> process reads the save-files
looking for block-ids to restore. It then connects to the respective database,
and requests Postgres to fetch the blocks into shared-buffers.</p>

<h2>Caveats</h2>

<ul>
<li>It saves the buffer information only when Postgres server is shutdown in normal mode.</li>
<li>It doesn&rsquo;t save/restore the filesystem/kernel&rsquo;s disk cache.</li>
</ul>

]]></content>
  </entry>
  
</feed>
