<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Postgres and other musings]]></title>
  <link href="http://gurjeet.singh.im/blog/atom.xml" rel="self"/>
  <link href="http://gurjeet.singh.im/blog/"/>
  <updated>2014-07-23T23:03:27+00:00</updated>
  <id>http://gurjeet.singh.im/blog/</id>
  <author>
    <name><![CDATA[Gurjeet Singh]]></name>
    <email><![CDATA[gurjeet@singh.im]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Announcing TPC-C.js; a Lightweight Implementation of TPC-C]]></title>
    <link href="http://gurjeet.singh.im/blog/2014/07/23/announcing-tpc-c-dot-js-a-lightweight-implementation-of-tpc-c/"/>
    <updated>2014-07-23T22:53:08+00:00</updated>
    <id>http://gurjeet.singh.im/blog/2014/07/23/announcing-tpc-c-dot-js-a-lightweight-implementation-of-tpc-c</id>
    <content type="html"><![CDATA[<p>I am glad to announce the <a href="https://github.com/gurjeet/DBYardstick/tree/v0.1.0/TPC-C">beta</a> release of TPC-C.js, which implements one of
the most popular database benchmarks, <a href="http://www.tpc.org/tpcc/default.asp">TPC-C</a>. It&rsquo;s not a coincidence that today
is also the <a href="http://www.tpc.org/information/sessions/sigmod/sld007.htm">22nd anniversary</a> of the TPC-C benchmark.</p>

<p>It currently supports <a href="http://www.postgresql.org/">Postgres</a> database, but can be easily extended to test
other database systems.</p>

<p>You might ask <em>&ldquo;Why another TPC-C implementation when we already have so many of
them?&rdquo;&ldquo;</em></p>

<p>Short answer: This one is very light on system resources, so you can</p>

<ol>
<li>Run the benchmark strictly adhering to the specification, and</li>
<li>Invest more in database hardware, rather than client hardware.</li>
</ol>


<p>Long answer: It&rsquo;s covered in the <a href="https://github.com/gurjeet/DBYardstick/tree/master/TPC-C#motivation">Motivation</a> section of TPC-C.js, which I&rsquo;ll
quote here:</p>

<blockquote><h1>Motivation</h1>

<p>The TPC-C benchmark drivers currently available to us, like TPCC-UVa, DBT2,
HammerDB, BenchmarkSQL, etc., all run one process (or thread) per simulated
client. Because the TPC-C benchmark specification limits the max tpmC metric
(transactions per minute of benchmark-C) from any single client to be 1.286 tpmC,
this means that to get a result of, say, 1 million tpmC we have to run about
833,000 clients. Even for a decent number as low as 100,000 tpmC, one has to run
83,000 clients.</p>

<p>Given that running a process/thread, even on modern operating systems, is a bit
expensive, it requires a big upfront investment in hardware to run the thousands
of clients required for driving a decent tpmC number. For example, the current
TPC-C record holder had to run 6.8 million clients to achieve 8.55 million tpmC,
and they used 16 high-end servers to run these clients, which cost them about
$ 220,000 (plus $ 550,000 in client-side software).</p>

<p>So, to avoid those high costs, these existing open-source implementations of
TPC-C compromise on the one of the core requirements of the TPC-C benchmark:
keying and thinking times. These implementations resort to just hammering the
SUT (system under test) with a constant barrage of transactions from a few
clients (ranging from 10-50).</p>

<p>So you can see that even though a decent modern database (running on a single
machine) can serve a few hundred clients simultaneously, it ends up serving
very few (10-50) clients. I strongly believe that this way the database is
not being tested to its full capacity; at least not as the TPC-C specification
intended.</p>

<p>The web-servers of yesteryears also suffer from the same problem; using one
process for each client request prohibits them from scaling, because the
underlying operating system cannot run thousands of processes efficiently. The
web-servers solved this problem (known as <a href="http://en.wikipedia.org/wiki/C10k_problem">c10k problem</a>) by using event-driven
architecture which is capable of handling thousands of clients using a single
process, and with minimal effort on the operating system&rsquo;s part.</p>

<p>So this implementation of TPC-C uses a similar architecture and uses <a href="http://nodejs.org/">NodeJS</a>,
the event-driven architecture, to run thousands of clients against a database.</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Postgres Hibernator: Reduce Planned Database Down Times]]></title>
    <link href="http://gurjeet.singh.im/blog/2014/04/30/postgres-hibernator-reduce-planned-database-down-times/"/>
    <updated>2014-04-30T17:23:57+00:00</updated>
    <id>http://gurjeet.singh.im/blog/2014/04/30/postgres-hibernator-reduce-planned-database-down-times</id>
    <content type="html"><![CDATA[<p><em>TL;DR</em>: Reduce planned database down times by about 97%, by using <a href="https://github.com/gurjeet/pg_hibernator">Postgres Hibernator</a>.</p>

<p>DBAs are often faced with the task of performing some maintenance on their database server(s) which requires shutting down the database. The maintenance may involve anything from a database minor-version upgrade, to a hardware upgrade. One ugly side-effect of restarting the database server/service is that all the data currently in database server&rsquo;s memory will be all lost, which was painstakingly fetched from disk and put there in response to application queries over time. And this data will have to be rebuilt as applications start querying database again. The query response times will be very high until all the &ldquo;hot&rdquo; data is fetched from disk and put back in memory again.</p>

<p>People employ a few tricks to get around this ugly truth, which range from running a <code>select * from app_table;</code>, to <code>dd if=table_file ...</code>, to using specialized utilities like <a href="https://github.com/klando/pgfincore">pgfincore</a> to prefetch data files into OS cache. Wouldn&rsquo;t it be ideal if the database itself could save and restore its memory contents across restarts!</p>

<p>The <a href="https://github.com/gurjeet/pg_hibernator">Postgres Hibernator</a> extension for <a href="http://www.postgresql.org">Postgres</a> performs the automatic save and restore of database buffers, integrated with database shutdown and startup, hence reducing the durations of database maintenance windows, in effect increasing the uptime of your applications.</p>

<p>Postgres Hibernator automatically saves the list of shared buffers to the disk on database shutdown, and automatically restores the buffers on database startup. This acts pretty much like your Operating System&rsquo;s hibernate feature, except, instead of saving the contents of the memory to disk, Postgres Hibernator saves just a list of block identifiers. And it uses that list after startup to restore the blocks from data directory into Postgres&#8217; <a href="http://www.postgresql.org/docs/current/static/runtime-config-resource.html#GUC-SHARED-BUFFERS">shared buffers</a>.</p>

<p>As explained in my <a href="http://gurjeet.singh.im/blog/2014/02/03/introducing-postgres-hibernator/">earlier post</a>, this extension is a set-it-and-forget-it solution, so, to get the benefits of this extension there&rsquo;s not much a DBA has to do, except install it.</p>

<p>Ideal database installations that would benefit from this extension would be the ones with a high cache-hit ratio. With Postgres Hibernator enabled, your database would start cranking pre-maintenance TPS (Transactions Per Second) within first couple of minutes after a restart.</p>

<p>As can be seen in the chart below, the database ramp-up time drops dramatically when Postgres Hibernator is enabled. The sooner the database TPS can reach the steady state, the faster your applications can start performing at full throttle.</p>

<p>The ramp-up time is even shorter if you wait for the Postgres Hibernator processes to end, before starting your applications.</p>

<h2>Sample Runs</h2>

<p><img src="http://gurjeet.singh.im/blog/../images/pg_hibernator_comparison.png" alt="Postgres Hibernator Comparison" />
<img src="http://gurjeet.singh.im/blog/../images/pg_hibernator_comparison_bars.png" alt="Postgres Hibernator Comparison Bars" /></p>

<p>As is quite evident, waiting for Postgres Hibernator to finish loading the data blocks before starting the application yeilds a 97% impprovement in database ramp-up time (2300 seconds to get to 122k TPS without Postgres Hibernator vs. 70 seconds).</p>

<h3>Details</h3>

<p>Please note that this is not a real benchmark, just something I developed to showcase this extension at its sweet spot.</p>

<p>The full source of this mini benchmark is available with the source code of the Postgres Hibernator, at its <a href="https://github.com/gurjeet/pg_hibernator">Git repo</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Hardware: MacBook Pro 9,1
</span><span class='line'>OS Distribution: Ubuntu 12.04 Desktop
</span><span class='line'>OS Kernel: Linux 3.11.0-19-generic
</span><span class='line'>RAM: 8 GB
</span><span class='line'>Physical CPU: 1
</span><span class='line'>CPU Count: 4
</span><span class='line'>Core Count: 8
</span><span class='line'>pgbench scale: 260 (~ 4 GB database)</span></code></pre></td></tr></table></div></figure>


<p>Before every test run, except the last (&lsquo;DB-only restart; No Hibernator&rsquo;), the Linux OS caches are dropped to simulate an OS restart.</p>

<p>In &lsquo;First Run&rsquo;, the Postgres Hibernator is enabled, but since this is the first ever run of the database, Postgres Hibernator doesn&rsquo;t kick in until shutdown, to save the buffer list.</p>

<p>In &lsquo;Hibernator w/ App&rsquo;, the application (pgbench) is started right after database restart. The Postgres Hibernator is restoring the data blocks to shared buffers while the application is also querying the database.</p>

<p>In the &lsquo;App after Hibernator&rsquo; case, the application is started <em>after</em> the Postgres Hibernator has finished reading database blocks. This took 70 seconds for reading the ~4 GB database.</p>

<p>In &lsquo;DB-only restart; No Hibernator` run, the OS caches are not dropped, but just the database service is restarted. This simulates database minor version upgrades, etc.</p>

<p>It&rsquo;s interesting to monitor the <code>bi</code> column in <code>vmstat 10</code> output, while these tests are running. In &lsquo;First Run&rsquo; and &lsquo;DB-only restart&rsquo; cases this column&rsquo;s values stayed between 2000 and 5000 until all data was in shared buffers, and then it dropped to zero (meaning that all data has been read from disk into shared buffers). In &lsquo;Hibernator w/ app&rsquo; case, this column&rsquo;s value ranges from 15,000 to 65,000, with an average around 25,000. it demonstrates that Postgres Hibernator&rsquo;s <code>Block Reader</code> process is aggressively reading the blocks from data directory into the shared buffers, but apparently not fast enough because the applicaion&rsquo;s queries are causing random reads from disk, which interfere with the sequential scans that Postgres Hibernator is trying to perform.</p>

<p>And finally, in &lsquo;App after Hibernator&rsquo; case, this column consistently shows values between 60,000 and 65,000, implying that in absence of simultaneous application load, the <code>Block Reader</code> can read data into shared buffers much faster.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Google Maps Boston Plane In-flight]]></title>
    <link href="http://gurjeet.singh.im/blog/2014/02/23/google-maps-boston-plane-in-flight/"/>
    <updated>2014-02-23T03:08:11+00:00</updated>
    <id>http://gurjeet.singh.im/blog/2014/02/23/google-maps-boston-plane-in-flight</id>
    <content type="html"><![CDATA[<!-- Since my blog is hosted as example.com/blog/ and images are under example.com/images/, and 
since (my version of) Octopress converts a leading slash into /blog, following is the site-agnostic
way of getting to the images.

IOW, /../images/abc.png will be converted to /blog/../images/abc.png -->


<p>I&rsquo;m not sure if I&rsquo;m the first one to spot this, but I just found a plane captured in-flight by <a href="https://maps.google.com/maps?q=Callahan+Tunnel,+Boston,+MA&amp;hl=en&amp;ll=42.339415,-71.016687&amp;spn=0.003489,0.00662&amp;sll=42.752811,-71.49671&amp;sspn=0.221846,0.42366&amp;z=18">Google Maps</a></p>

<p><img src="http://gurjeet.singh.im/blog/../images/Google_Maps_Boston_Plane_in-flight.png" alt="pic" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introducing Postgres Hibernator]]></title>
    <link href="http://gurjeet.singh.im/blog/2014/02/03/introducing-postgres-hibernator/"/>
    <updated>2014-02-03T21:28:46+00:00</updated>
    <id>http://gurjeet.singh.im/blog/2014/02/03/introducing-postgres-hibernator</id>
    <content type="html"><![CDATA[<p>As it must have been obvious from my last <a href="http://gurjeet.singh.im/blog/2014/01/21/hibernating-and-restoring-postgres-buffer-cache/">post</a> that I
wasn&rsquo;t really pleased by the amount of work needed to implement hibernation of
Postgres shared-buffers, so I set out to implement a seamless Postgres hibernation
solution.</p>

<p>A couple of hours ago I <a href="http://www.postgresql.org/message-id/CABwTF4Ui_anAG+ybseFunAH5Z6DE9aw2NPdy4HryK+M5OdXCCA@mail.gmail.com">published</a> the <a href="http://www.postgresql.org">Postgres</a>/<a href="http://www.enterprisedb.com">EDB</a> extension I had been
working on for last 10 days or so, in my spare time. Following are the contents of the
README file from the <a href="https://github.com/gurjeet/pg_hibernate">extension</a>.</p>

<h1>Postgres Hibernator</h1>

<p>This Postgres extension is a set-it-and-forget-it solution to save and restore
the Postgres shared-buffers contents, across Postgres server restarts.</p>

<p>For some details on the internals of this extension, also see the <a href="http://www.postgresql.org/message-id/CABwTF4Ui_anAG+ybseFunAH5Z6DE9aw2NPdy4HryK+M5OdXCCA@mail.gmail.com">proposal</a>
email to Postgres hackers&#8217; mailing list.</p>

<h2>Why</h2>

<p>When a database server is shut down, for any reason (say, to apply patches, for
scheduled maintenance, etc.), the active data-set that is cached in memory by
the database server is lost. Upon starting up the server again, the database
server&rsquo;s cache is empty, and hence almost all application queries respond slowly
because the server has to fetch the relevant data from the disk. It takes quite a
while for the server to bring the cache back to similar state as before the server
shutdown.</p>

<p>The duration for which the server is building up caches, and trying to reach its
optimal cache performance is called ramp-up time.</p>

<p>This extension is aimed at reducing the ramp-up time of Postgres servers.</p>

<h2>How</h2>

<p>Compile and install the extension (of course, you&rsquo;d need Postgres installation or
source code):</p>

<pre><code>$ make -C pg_hibernate/ install
</code></pre>

<p>Then.</p>

<ol>
<li>Add <code>pg_hibernate</code> to the <code>shared_preload_libraries</code> variable in <code>postgresql.conf</code> file.</li>
<li>Restart the Postgres server.</li>
<li>You are done.</li>
</ol>


<h2>How it works</h2>

<p>This extension uses the <code>Background Worker</code> infrastructure of Postgres, which was
introduced in Postgres 9.3. When the server starts, this extension registers
background workers; one for saving the buffers (called <code>Buffer Saver</code>) when the
server shuts down, and one for each database in the cluster (called <code>Block Readers</code>)
for restoring the buffers saved during previous shutdown.</p>

<p>When the Postgres server is being stopped/shut down, the <code>Buffer Saver</code> scans the
shared-buffers of Postgres, and stores the unique block identifiers of each cached
block to the disk (with some optimizatins). This information is saved under the
<code>$PGDATA/pg_hibernate/</code> directory. For each of the database whose blocks are
resident in shared buffers, one file is created; for eg.:
<code>$PGDATA/pg_hibernate/2.postgres.save</code>.</p>

<p>During the next startup sequence, the <code>Block Reader</code> threads are registerd, one for
each file present under <code>$PGDATA/pg_hibernate/</code> directory. When the Postgres server
has reached stable state (that is, it&rsquo;s ready for database connections), these
<code>Block Reader</code> processes are launched. The <code>Block Reader</code> process reads the save-files
looking for block-ids to restore. It then connects to the respective database,
and requests Postgres to fetch the blocks into shared-buffers.</p>

<h2>Caveats</h2>

<ul>
<li>It saves the buffer information only when Postgres server is shutdown in normal mode.</li>
<li>It doesn&rsquo;t save/restore the filesystem/kernel&rsquo;s disk cache.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hibernating and Restoring Postgres Buffer Cache]]></title>
    <link href="http://gurjeet.singh.im/blog/2014/01/21/hibernating-and-restoring-postgres-buffer-cache/"/>
    <updated>2014-01-21T03:01:45+00:00</updated>
    <id>http://gurjeet.singh.im/blog/2014/01/21/hibernating-and-restoring-postgres-buffer-cache</id>
    <content type="html"><![CDATA[<p>With the introduction of <a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=c32afe53c2e87a56e2ff930798a5588db0f7a516"><code>pg_prewarm</code></a> extension in Postgres,
it has become very easy to save and restore the contents of Postgres server&rsquo;s
buffer cache across a server restart. The tables, indexes and other on-disk data
structures (like <a href="http://www.postgresql.org/docs/9.3/static/storage-toast.html">TOAST</a> data, Visibility Map, etc.) are cached in
shared-buffers before they can be read or modified by user queries. Hence, by
saving the shared-buffers before a server restart, and restoring those buffers
after the restart, you can expect to reduce the ramp-up time, and hence expect
peak performance almost right off the bat.</p>

<p>Following is a brain-dead way of implementing hibernation, and I am sure it can
be optimized to reduce the number of calls to <code>pg_prewarm</code>, since it allows the
caller to specify a range of blocks to be loaded, and I&rsquo;m asking it to load just
one block per call.</p>

<p>We&rsquo;re going to use <a href="http://www.postgresql.org/docs/9.3/static/pgbuffercache.html"><code>pg_buffercache</code></a> to extract the list of
buffers currently loaded in buffer cache, and after a server restart, <code>pg_prewarm</code>
will be used to load those buffers back in.</p>

<h2>Declare environment variables to be used by scripts</h2>

<pre><code>HIBERNATE_DESTINATION=$HOME/pg_hibernate/

export PGUSER=postgres
PSQL="psql"
PSQL_TEMPL_DB="$PSQL -d template1"
PSQL_PG_DB="$PSQL -d postgres"

PSQL_TEMPL_NOFLUFF="$PSQL_TEMPL_DB -A -t"
PSQL_PG_NOFLUFF="$PSQL_PG_DB -A -t"
</code></pre>

<h2>Prepare the databases</h2>

<p>This is a one time operation. We avoid installing anything into <code>template0</code>
database, since it is a read-only database. But we do consciously install the
extension into <code>template1</code> database; this is so that any new databases created
after this point will get this extension pre-installed.</p>

<h3>Install extensions</h3>

<pre><code>for db in $($PSQL_TEMPL_NOFLUFF -c 'select datname from pg_database where datname &lt;&gt; $$template0$$'); do
  echo Installing pg_prewarm in $db
  $PSQL_TEMPL_DB -c 'create extension if not exists pg_prewarm;'
done

echo Installing pg_buffercache extension in postgres database
$PSQL_PG_DB -c 'create extension if not exists pg_buffercache;'
</code></pre>

<h2>Save buffer information</h2>

<p>We are actually generating a <code>psql</code> script, that can be later fed to <code>psql</code>, as is.</p>

<pre><code>mkdir -p $HIBERNATE_DESTINATION

for db in $($PSQL_TEMPL_NOFLUFF -c 'select datname from pg_database where datname &lt;&gt; $$template0$$'); do
  $PSQL_PG_NOFLUFF -c 'select    $q$select pg_prewarm((    select    oid
                                                         from      pg_class
                                                         where     pg_relation_filenode(oid) = $q$ || relfilenode || $q$)::regclass,
                                                     $$buffer$$, $q$
                                                     || case relforknumber
                                                        when 0 then $q$$$main$$$q$
                                                        when 1 then $q$$$fsm$$$q$
                                                        when 2 then $q$$$vm$$$q$
                                                        when 3 then $q$$$init$$$q$
                                                        end || $q$, $q$
                                                     || relblocknumber || $q$, $q$
                                                     || relblocknumber || $q$);$q$
                                    from     pg_buffercache
                                    where    reldatabase = (select    oid
                                                            from      pg_database
                                                            where     datname = $$'${db}'$$)
                                    order by relfilenode, relforknumber, relblocknumber;' &gt; $HIBERNATE_DESTINATION/${db}.save
done
</code></pre>

<h2>Restore the buffers</h2>

<p>Ideally, this would be performed after a server restart, but there&rsquo;s no harm in
doing it without the restart either (except the performance implications, if
doing this eveicts buffers currently in use by other connections ;). Note that
if some table/index&rsquo;s underlying file storage has been renamed by the server
since we extracted the buffers list, some of these calls will return with <code>ERROR</code>.</p>

<p>We connect to each database, and simply feed the script generated earlier, into <code>psql</code>.</p>

<pre><code>for db in $($PSQL_TEMPL_NOFLUFF -c 'select datname from pg_database where datname &lt;&gt; $$template0$$'); do
  if [ ! -e $HIBERNATE_DESTINATION/${db}.save ]; then
    continue
  fi
  $PSQL -d $db -f $HIBERNATE_DESTINATION/${db}.save
done
</code></pre>

<p>At this point, the Postgres shared-buffers should contain all the buffers that
were present when we extracted the buffer list from <code>pg_buffercache</code>.</p>

<p>PS: When reviewing the <code>pg_prewarm</code> code, I did not think through the user-experience
aspect of this extension. But after it was committed, the more I thought about
how it&rsquo;s going to be used, the less appealing this solution became. As is evident
from above, the administrator needs the help of two extensions, and then a storage
location where to store the list of buffers. Ideally, as a DBA, I would like to
see a feature which doesn&rsquo;t require me to muck around with catalogs etc. I have
a design for such an extension, and may start coding it some time soon.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Understanding Postgres Parameter Context]]></title>
    <link href="http://gurjeet.singh.im/blog/2014/01/07/understanding-postgres-parameter-context/"/>
    <updated>2014-01-07T21:55:21+00:00</updated>
    <id>http://gurjeet.singh.im/blog/2014/01/07/understanding-postgres-parameter-context</id>
    <content type="html"><![CDATA[<p>Postgres&#8217; parameters have an associated context, which determines when that
parameter can be changed.</p>

<p>You can see the context of every parameter using the following query. A sample
output is also shown.</p>

<pre><code>select name, context from pg_settings order by category;

              name               |  context   
---------------------------------+------------
 autovacuum_freeze_max_age       | postmaster
 autovacuum_max_workers          | postmaster
 autovacuum_vacuum_threshold     | sighup
 ...
 IntervalStyle                   | user
 ...
 server_encoding                 | internal
 ...
 lc_messages                     | superuser
 ...
 local_preload_libraries         | backend
 ...
</code></pre>

<p>The possible values of context are:</p>

<ul>
<li>internal (called <code>PGC_INTERNAL</code> in source code)</li>
<li>postmaster (<code>PGC_POSTMASTER</code>)</li>
<li>sighup (<code>PGC_SIGHUP</code>)</li>
<li>backend (<code>PGC_BACKEND</code>)</li>
<li>superuser (<code>PGC_SUSET</code>)</li>
<li>user (<code>PGC_USERSET</code>)</li>
</ul>


<p>The above list is in order of when a parameter can be set; if a parameter can be
changed in a certain context, then it can be changed at any of the earlier
contexts as well.</p>

<h3>internal</h3>

<p>The <code>internal</code> parameters cannot be changed; these are usually compile-time
constants. If you want to change any of these, you&rsquo;ll have to change it in
Postgres source code and compile a new set of Postgres executables.</p>

<h3>postmaster</h3>

<p>The <code>postmaster</code> parameters can be set at Postgres startup, or during source code
compilation. (Postmaster is the parent process of all the Postgres processes,
hence the context&rsquo;s name).</p>

<p>These parameters can be set in the <code>postgresql.conf</code> file or on the command-line
when starting the Postgres server.</p>

<h3>sighup</h3>

<p>The <code>sighup</code> parameters can be changed while the server is running, at Postgres
startup, or during code compilation.</p>

<p>To change such a parameter, you can change it in the <code>postgresql.conf</code> file and send a
<code>SIGHUP</code> signal to the Postmaster process. An easy way to send the <code>SIGHUP</code> signal
to the Postmaster process is to use <code>pg_ctl</code> or your distribution&rsquo;s Postgres-init
script, like so:</p>

<pre><code>pg_clt -D $PGDATA reload
</code></pre>

<p>OR</p>

<pre><code>sudo service postgresql-9.3 reload
</code></pre>

<h3>backend</h3>

<p>The <code>backend</code> parameters can be changed/set while making a new connection to
Postgres, and never after that (and these can be changed by <code>SIGHUP</code>, at Postgres
startup, or during code compilation).</p>

<p>Usually the applications set these parameters while making the initial connection.</p>

<p>An example is the <code>local_preload_libraries</code> parameter. Say, if you want to try a
plugin for just one session, then you can initiate a <code>psql</code> session, with that
plugin loaded for the connection, like so:</p>

<pre><code>PGOPTIONS="-c local_preload_libraries=my_plugin" psql
</code></pre>

<p>The above method of changing parameters is possible for any application that
uses <code>libpq</code> library to connect to Postrges (for eg. <a href="http://pgadmin.org/">pgAdmin</a>), since the <code>PGOPTIONS</code>
environment variable is recognized and honored by <code>libpq</code>. Other applications/libraries
may have their own methods to allow changing parameters during connection initiation.</p>

<h3>superuser</h3>

<p>To change a <code>superuser</code> parameter, one needs to have <code>superuser</code> privileges in Postgres.
These parameters can be changed while a session is in progress, during a backend
startup, using <code>SIGHUP</code>, at Postgres startup, or during code compilation.</p>

<p>Note that normal users cannot change these parameters.</p>

<h3>user</h3>

<p>The parameters with <code>user</code> context can be changed by any user, at any time, to
affect the current session they are connected to. Needless to say that since this
is the last context in the list, a parameter that is marked as <code>user</code> context, can be changed
using any of the methods shown for the other contexts.</p>

<p>The <code>SET</code> command can be used to change a <code>user</code> context parameter&rsquo;s value, for eg.:</p>

<pre><code>SET work_mem = '32 MB';
</code></pre>

<h2>Using context in queries</h2>

<p>Although, as explained above, there is a certain order in the values of <code>context</code>,
there is no built-in way for one to see this order, and exploit that knowledge
using queries.</p>

<p>Say, if one wants to see a list of all parameters that cannot be
changed by a normal user, there&rsquo;s no straightforward way to do it. To that end,
I create the following <code>enum</code> type and use it in queries to extract that information easily:</p>

<pre><code>create type guc_context as enum (
    'internal',
    'postmaster',
    'sighup',
    'backend',
    'superuser',
    'user');

select name as cannot_be_changed_by_user, context
from pg_settings
where context::guc_context &lt; 'user';
</code></pre>

<p>Other useful information that can now be easily extracted using this <code>enum</code>:</p>

<pre><code>select name as parameter,
    context_enum &gt; 'internal' as can_be_changed,
    context_enum = 'postmaster' as change_requires_restart,
    context_enum &gt;= 'sighup' as can_be_changed_by_reload
from (select name, context::guc_context as context_enum
    from pg_settings) as v;

            parameter            | can_be_changed | change_requires_restart | can_be_changed_by_reload 
---------------------------------+----------------+-------------------------+--------------------------
 allow_system_table_mods         | t              | t                       | f
 application_name                | t              | f                       | t
 archive_command                 | t              | f                       | t
 archive_mode                    | t              | t                       | f
 archive_timeout                 | t              | f                       | t
 array_nulls                     | t              | f                       | t
 authentication_timeout          | t              | f                       | t
 ...
</code></pre>
]]></content>
  </entry>
  
</feed>
